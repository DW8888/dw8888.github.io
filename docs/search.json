[
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Darwhin Gomez",
    "section": "",
    "text": "Print / Save as PDF\n\nData Engineer | Data Scientist\nNew York, NY\nðŸ“§ darwhin88@gmail.com\nðŸ”— https://linkedin.com/in/darwhin-gomez\nðŸ’» https://github.com/dw8888\n\n\nSUMMARY\nData engineer and data scientist with a Master of Science in Data Science and Machine Learning (GPA 4.0) and hands-on experience designing data pipelines, analytics workflows, and ML-enabled systems. Former AWS Solutions Architect Intern, where I built scalable, cloud-based analytics and NLP pipelines processing millions of records. Strong foundation in Python, SQL, cloud services, and applied machine learning, with a focus on reproducibility, evaluation, and real-world decision-making. Seeking a junior role to contribute to production data systems while continuing to grow as an engineer.\n\n\n\nEDUCATION\nCity University of New York â€“ School of Professional Studies\nMaster of Science, Data Science and Machine Learning â€” Jan 2026\nGPA: 4.0\nCity University of New York â€“ New York City College of Technology\nBachelor of Technology, Computer Information Systems â€” Jun 2024\nGPA: 3.78 | Deanâ€™s List (5Ã—) | National Honor Society\n\n\n\nCERTIFICATIONS\n\nAWS Certified Solutions Architect â€“ Associate\n\nAWS Certified AI Practitioner â€“ Generative AI\n\nGoogle Cybersecurity Certificate\n\nGoogle IT Support\n\n\n\n\nPROFESSIONAL EXPERIENCE\n\nSolutions Architect Intern â€” Amazon Web Services (AWS)\nArlington, VA | Jun 2025 â€“ Sep 2025\n\nDesigned and evaluated secure, scalable AWS architectures, analyzing trade-offs across cost, performance, and operational complexity.\n\nBuilt end-to-end data analytics pipelines using Athena, Glue, SQL, and serverless components.\n\nDeveloped a fully automated sentiment analysis pipeline processing millions of text records, reducing feedback-to-action latency by approximately 5Ã— while maintaining full traceability.\n\nAutomated a previously 100% manual QA workflow, enabling analysts to focus on interpretation rather than processing.\n\nApplied NLP techniques and prompt-design strategies using foundation models via Amazon Bedrock.\n\nDesigned and deployed Amazon QuickSight dashboards to monitor sentiment trends and validate insights.\n\nPresented architectural designs and cost analyses to technical and non-technical stakeholders.\n\n\n\n\nSoftware Developer Intern â€” WOPLLI Technologies\nRemote, NY | Jun 2024 â€“ Sep 2024\n\nDesigned system and data-flow architecture diagrams aligned with data governance and security requirements.\n\nIntegrated and tested APIs supporting decentralized identity workflows on the Ethereum network.\n\nEvaluated system behavior and edge cases to ensure correctness and reliability.\n\n\n\n\nIT Instructor Assistant â€” RF CUNY & Generation USA (Choose-U)\nRemote, NY | Feb 2022 â€“ Aug 2023\n\nDelivered 36 instructional sessions covering IT support, Linux, PowerShell, and scripting to 200+ learners.\n\nTracked learner engagement using Excel and Google Sheets, producing summary statistics and progress reports.\n\nUsed performance data to refine instruction, contributing to a 96% cohort graduation rate.\n\n\n\n\n\nPROJECTS\n\nAlfred â€” Agentic Job Search Assistant | Sep 2025 â€“ Present\n\nBuilt a personal agentic job search assistant using FastAPI (Python) and OpenAI GPT models.\n\nImplemented a Retrieval-Augmented Generation (RAG) system using PostgreSQL with pgvector.\n\nApplied cosine similarity to match candidates to job postings, reducing application time by ~60%.\n\nConducted LLM evaluation using human-in-the-loop and LLM-as-judge techniques.\n\n\n\n\n\nTECHNICAL SKILLS\nLanguages & Programming\nPython (pandas, scikit-learn, PyTorch), R, SQL\nData & Analytics Tools\nExcel, PowerPoint, Google Sheets, Tableau, Power BI\nCloud & Data Platforms\nAWS (Athena, Glue, Bedrock, QuickSight), Databricks (familiarity)\nAI / ML Platforms\nOpenAI, Claude, Llama, Google GenAI\nTechniques & Methods\nETL, NLP, Classification, Regression, Ensemble Methods (CART), Forecasting (ARIMA), Anomaly Detection, Hypothesis Testing, EDA, PCA, Clustering\nDevelopment Environments\nJupyter, Colab, VS Code, Rstudio, Pycharm",
    "crumbs": [
      "Resume"
    ]
  },
  {
    "objectID": "resume.html#darwhin-gomez",
    "href": "resume.html#darwhin-gomez",
    "title": "Darwhin Gomez",
    "section": "Darwhin Gomez",
    "text": "Darwhin Gomez\nData Engineer | Data Scientist\nNew York, NY\nðŸ“§ darwhin88@gmail.com\nðŸ”— https://linkedin.com/in/darwhin-gomez\nðŸ’» https://github.com/dw8888\n\n\nSUMMARY\nData engineer and data scientist with a Master of Science in Data Science and Machine Learning (GPA 4.0) and hands-on experience designing data pipelines, analytics workflows, and ML-enabled systems. Former AWS Solutions Architect Intern, where I built scalable, cloud-based analytics and NLP pipelines processing millions of records. Strong foundation in Python, SQL, cloud services, and applied machine learning, with a focus on reproducibility, evaluation, and real-world decision-making. Seeking a junior role to contribute to production data systems while continuing to grow as an engineer.\n\n\n\nEDUCATION\nCity University of New York â€“ School of Professional Studies\nMaster of Science, Data Science and Machine Learning â€” Jan 2026\nGPA: 4.0\nCity University of New York â€“ New York City College of Technology\nBachelor of Technology, Computer Information Systems â€” Jun 2024\nGPA: 3.78 | Deanâ€™s List (5Ã—) | National Honor Society\n\n\n\nCERTIFICATIONS\n\nAWS Certified Solutions Architect â€“ Associate\n\nAWS Certified AI Practitioner â€“ Generative AI\n\nGoogle Cybersecurity Certificate\n\nGoogle IT Support\n\n\n\n\nPROFESSIONAL EXPERIENCE\n\nSolutions Architect Intern â€” Amazon Web Services (AWS)\nArlington, VA | Jun 2025 â€“ Sep 2025\n\nDesigned and evaluated secure, scalable AWS architectures, analyzing trade-offs across cost, performance, and operational complexity.\n\nBuilt end-to-end data analytics pipelines using Athena, Glue, SQL, and serverless components.\n\nDeveloped a fully automated sentiment analysis pipeline processing millions of text records, reducing feedback-to-action latency by approximately 5Ã— while maintaining full traceability.\n\nAutomated a previously 100% manual QA workflow, enabling analysts to focus on interpretation rather than processing.\n\nApplied NLP techniques and prompt-design strategies using foundation models via Amazon Bedrock.\n\nDesigned and deployed Amazon QuickSight dashboards to monitor sentiment trends and validate insights.\n\nPresented architectural designs and cost analyses to technical and non-technical stakeholders.\n\n\n\n\nSoftware Developer Intern â€” WOPLLI Technologies\nRemote, NY | Jun 2024 â€“ Sep 2024\n\nDesigned system and data-flow architecture diagrams aligned with data governance and security requirements.\n\nIntegrated and tested APIs supporting decentralized identity workflows on the Ethereum network.\n\nEvaluated system behavior and edge cases to ensure correctness and reliability.\n\n\n\n\nIT Instructor Assistant â€” RF CUNY & Generation USA (Choose-U)\nRemote, NY | Feb 2022 â€“ Aug 2023\n\nDelivered 36 instructional sessions covering IT support, Linux, PowerShell, and scripting to 200+ learners.\n\nTracked learner engagement using Excel and Google Sheets, producing summary statistics and progress reports.\n\nUsed performance data to refine instruction, contributing to a 96% cohort graduation rate.\n\n\n\n\n\nPROJECTS\n\nAlfred â€” Agentic Job Search Assistant | Sep 2025 â€“ Present\n\nBuilt a personal agentic job search assistant using FastAPI (Python) and OpenAI GPT models.\n\nImplemented a Retrieval-Augmented Generation (RAG) system using PostgreSQL with pgvector.\n\nApplied cosine similarity to match candidates to job postings, reducing application time by ~60%.\n\nConducted LLM evaluation using human-in-the-loop and LLM-as-judge techniques.\n\n\n\n\n\nTECHNICAL SKILLS\nLanguages & Programming\nPython (pandas, scikit-learn, PyTorch), R, SQL\nData & Analytics Tools\nExcel, PowerPoint, Google Sheets, Tableau, Power BI\nCloud & Data Platforms\nAWS (Athena, Glue, Bedrock, QuickSight), Databricks (familiarity)\nAI / ML Platforms\nOpenAI, Claude, Llama, Google GenAI\nTechniques & Methods\nETL, NLP, Classification, Regression, Ensemble Methods (CART), Forecasting (ARIMA), Anomaly Detection, Hypothesis Testing, EDA, PCA, Clustering\nDevelopment Environments\nJupyter, Colab, VS Code, Rstudio, Pycharm"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Profile\n\n\nDarwhin Gomez is a data and software engineer with a strong foundation in data engineering, analytics, and machine learning. He recently completed a Master of Science in Data Science from the School of Professional Studies at the City University of New York, where he worked extensively with real-world datasets, statistical modeling, and end-to-end data workflows. His experience includes building data pipelines, performing feature engineering, and developing machine learning models using Python, SQL, and R, with an emphasis on reproducibility, evaluation rigor, and practical decision-making.\nIn 2025, he interned as a Solutions Architect at Amazon Web Services (AWS), where he designed and deployed AI-powered solutions across multiple AWS services with a focus on scalability, reliability, and performance. In 2024, he interned as a Software and Product Developer at WOPLLI, contributing to research and proof-of-concept development for Decentralized Identity (DID) and Verifiable Credentials (VC) systems, as well as building a web application for issuing and managing digital credentials. He also served as a Help Desk Intern with the Office of Technology for the Mayor of New York City, providing technical support and troubleshooting for city employees.\nWhile pursuing his Bachelorâ€™s degree in Computer Systems Technology from the New York City College of Technology, Darwhin worked as a Teaching Assistant for IT courses through the Research Foundation of the City University of New York, supporting students in professional IT systems and support coursework.\nOutside of his professional work, Darwhin is a devoted husband and father of two. He enjoys spending time with his family, exploring emerging technologies, reading epic fantasy and science fiction, and following New York sports.",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Darwhin Gomez",
    "section": "",
    "text": "I am a data professional with a strong foundation in data engineering, analytics, and machine learning. I recently completed a Master of Science in Data Science, where I worked extensively with real-world datasets, statistical modeling, and end-to-end data workflows.\nLearn more about me through the links below, and explore my projects and certifications for examples of my work and technical expertise.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#data-engineer-data-scientist",
    "href": "index.html#data-engineer-data-scientist",
    "title": "Darwhin Gomez",
    "section": "Data Engineer | Data Scientist",
    "text": "Data Engineer | Data Scientist\nI am a data professional with a strong foundation in data engineering, analytics, and machine learning. I recently completed a Master of Science in Data Science, where I worked extensively with real-world datasets, statistical modeling, and end-to-end data workflows.\nLearn more about me through the links below, and explore my projects and certifications for examples of my work and technical expertise.\n\n\nLinks\n\nAbout: About me\nLinkedIn: https://www.linkedin.com/in/darwhin-gomez\n\nGitHub: https://github.com/dw8888\n\nResume: Resume\nCertifications: Certifications\n\n\n\n\nFeatured Projects\nA selection of projects that demonstrate my experience in data engineering, analytics, machine learning, and AI-powered systems.\n(See the Projects tab for full details.)\n\n\n\nAgent0 â€“ RAG-Grounded Analytics Agent\nAgent0 is a retrieval-augmented analytics agent that converts natural language questions into safe, explainable SQL insights.\nThe system combines a star-schema analytics warehouse with business-rule and metadata retrieval (RAG) to ensure that generated queries are grounded in approved definitions before execution. Agent0 uses an agentic workflow to decide when to retrieve context, execute SQL, or perform post-processing in Python.\nKey ideas behind Agent0: - Semantic grounding via RAG to reduce SQL hallucinations\n- Clear separation of reasoning, retrieval, and execution\n- OLAP-first design with a migration path to Postgres or cloud warehouses\n- Guardrails and auditability beyond prompt-only approaches\nTech stack: DuckDB (prototype OLAP) â†’ Postgres/Redshift-style warehouse, FAISS (vector retrieval), LangChain (agent orchestration), Python (analytics + SQL), LLM-agnostic (currently using GPT-4o via OpenAI API).\n\nRepo: https://github.com/DW8888/agent0\n\nArchitecture & design: Agent0 details\n\n\n\n\nAlfred â€“ AI Job Assistant (RAG)\nAlfred originated as both a practical solution to streamline my job search and as a Masterâ€™s capstone project. It is a RAG-powered job application assistant that generates tailored resumes and cover letters based on user profiles and job descriptions.\nKey ideas behind Alfred: - Uses vector databases and cosine similarity to retrieve relevant profile and job information\n- Leverages LLMs to generate customized application materials\n- Built with FastAPI and PostgreSQL/pgvector for efficient retrieval\n- Includes a React/Next.js frontend with a FastAPI backend for end-to-end user interaction\nTech stack: FastAPI, PostgreSQL/pgvector, Python, Next.js/React (TypeScript), OpenAI-powered RAG workflows.\n\n\n\nFraud Isolation Model\nThis project was developed as part of a case study in my data science program, where I built an Isolation Forest model to detect fraudulent financial transactions.\nThe notebook includes data preprocessing, exploratory data analysis, feature engineering, model training, and evaluation â€” demonstrating my ability to apply machine learning techniques to real-world problems.\nTech stack: Python, Jupyter Notebook, scikit-learn, pandas, matplotlib, seaborn.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "certifications.html",
    "href": "certifications.html",
    "title": "Certifications",
    "section": "",
    "text": "AWS Certified AI Practitioner\nAWS Certified Solutions Architect â€“ Associate\nAWS Certified Cloud Practitioner\nDataCamp AI Fundamentals\nGoogle Cybersecurity\nGoogle IT Support\nNetwork Assurance\nSecurity Fundamentals",
    "crumbs": [
      "Certifications"
    ]
  },
  {
    "objectID": "certifications.html#professional-certifications",
    "href": "certifications.html#professional-certifications",
    "title": "Certifications",
    "section": "",
    "text": "AWS Certified AI Practitioner\nAWS Certified Solutions Architect â€“ Associate\nAWS Certified Cloud Practitioner\nDataCamp AI Fundamentals\nGoogle Cybersecurity\nGoogle IT Support\nNetwork Assurance\nSecurity Fundamentals",
    "crumbs": [
      "Certifications"
    ]
  },
  {
    "objectID": "index.html#links",
    "href": "index.html#links",
    "title": "Darwhin Gomez",
    "section": "### Links",
    "text": "### Links\n\nLinkedIn: https://www.linkedin.com/in/darwhin-gomez\n\nGitHub: https://github.com/dw8888\n\nResume: Resume\nCertifications: Certifications\n\n\n\nFeatured Projects\nA selection of projects demonstrating data pipelines, modeling, and applied analytics.\n(See the Projects tab for full details.)\nAgent0 is a RAG-grounded analytics agent that converts natural language questions into safe, explainable SQL insights.\nThe system combines a star-schema analytics warehouse with business-rule and metadata retrieval (RAG) to ensure that generated queries are grounded in approved definitions before execution. Agent0 uses an agentic workflow to decide when to retrieve context, execute SQL, or perform post-processing in Python.\nKey ideas behind Agent0: - Semantic grounding via RAG to reduce SQL hallucinations - Clear separation of reasoning, retrieval, and execution - OLAP-first design with a straightforward migration path to Postgres or cloud warehouses - Guardrails and auditability over prompt-only approaches\nTech stack: DuckDB (OLAP)â€”&gt;&gt; Now Postgres Redshift, FAISS (RAG), LangChain (agent orchestration), Python (SQL + analytics), llm agnostic currently using GPT-4-0 from openai.\n\nRepo: https://github.com/DW8888/agent0\n\nArchitecture & design: Agent0 details\nAlfred â€“ AI Job Assistant (RAG)\nFastAPI-based system using PostgreSQL/pgvector and LLMs to generate tailored resumes and cover letters.\nBank Marketing Model Experiments\nComparative modeling using Decision Trees, Random Forests, AdaBoost, and SVMs with rigorous evaluation metrics.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#ai-solutions-engineer-cloud-data-systems",
    "href": "index.html#ai-solutions-engineer-cloud-data-systems",
    "title": "Darwhin Gomez",
    "section": "",
    "text": "I am a data professional with a strong foundation in data engineering, analytics, and machine learning. I recently completed a Master of Science in Data Science, where I worked extensively with real-world datasets, statistical modeling, and end-to-end data workflows.\nLearn more about me through the links below, and explore my projects and certifications for examples of my work and technical expertise.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "alfred_flow.html",
    "href": "alfred_flow.html",
    "title": "Alfred Architecture",
    "section": "",
    "text": "flowchart TD\n\n    subgraph UI[\"Next.js / React Dashboard\"]\n        A[Operator Actions]\n    end\n\n    subgraph API[\"FastAPI Backend (Python)\"]\n        R1[Jobs Router]\n        R2[Artifacts & Profiles]\n        R3[Search + Persona Resumes]\n        R4[GitHub Summaries]\n    end\n\n    subgraph Agents[\"Background Agents\"]\n        AF[Job Fetcher]\n        AM[Job Matcher]\n        AR[Resume Agent]\n        AC[Cover Letter Agent]\n        AG[GitHub Ingestion Agent]\n    end\n\n    subgraph Queues[\"JSON Queues & State Files\"]\n        Q1[job_fetcher_state.json]\n        Q2[state_job_matcher.json]\n        Q3[resume_queue.json]\n        Q4[cover_letter_queue.json]\n        Q5[github_ingestion_state.json]\n    end\n\n    subgraph DB[\"PostgreSQL + pgvector\"]\n        T1[(jobs)]\n        T2[(artifacts)]\n        T3[(generated_artifacts)]\n    end\n\n    subgraph External[\"External Services\"]\n        JAPI[(Job APIs / RSS / Boards)]\n        GH[(GitHub Repos)]\n        LLM[(OpenAI GPT Models)]\n    end\n\n    subgraph Infra[\"Infrastructure\"]\n        DKR[docker-compose Postgres]\n        AWS[AWS scaffolding (future ECS/RDS)]\n    end\n\n    %% UI &lt;-&gt; API (connect to actual nodes, not the subgraph name)\n    A --&gt;|HTTP requests| R1\n    R1 --&gt;|REST/WebSockets| A\n\n    %% API &lt;-&gt; DB\n    R1 --&gt; T1\n    R2 --&gt; T2\n    R3 --&gt; T3\n\n    %% Job ingestion pipeline\n    AF --&gt;|gets jobs| JAPI\n    AF --&gt; Q1\n    Q1 --&gt; AM\n    AM --&gt; Q2\n    AM --&gt;|strong matches| Q3\n    Q3 --&gt; AR\n    AR --&gt; Q4\n    Q4 --&gt; AC\n\n    %% Agents write to DB\n    AF --&gt; T1\n    AM --&gt; T1\n    AR --&gt; T3\n    AC --&gt; T3\n\n    %% GitHub ingestion\n    AG --&gt; GH\n    AG --&gt; Q5\n    Q5 --&gt; R4\n    R4 --&gt; LLM\n\n    %% LLM usage\n    AR --&gt;|prompting| LLM\n    AC --&gt;|prompting| LLM\n    AM --&gt;|LLM skill check| LLM\n\n    %% Infrastructure\n    DKR --&gt; T1\n    DKR --&gt; T2\n    DKR --&gt; T3\n\n    %% Correct dotted-arrow labeling syntax\n    AWS -.-&gt;|future deploy| R1\n    AWS -.-&gt;|future deploy| AF"
  },
  {
    "objectID": "alfred_flow.html#system-architecture",
    "href": "alfred_flow.html#system-architecture",
    "title": "Alfred Architecture",
    "section": "",
    "text": "flowchart TD\n\n    subgraph UI[\"Next.js / React Dashboard\"]\n        A[Operator Actions]\n    end\n\n    subgraph API[\"FastAPI Backend (Python)\"]\n        R1[Jobs Router]\n        R2[Artifacts & Profiles]\n        R3[Search + Persona Resumes]\n        R4[GitHub Summaries]\n    end\n\n    subgraph Agents[\"Background Agents\"]\n        AF[Job Fetcher]\n        AM[Job Matcher]\n        AR[Resume Agent]\n        AC[Cover Letter Agent]\n        AG[GitHub Ingestion Agent]\n    end\n\n    subgraph Queues[\"JSON Queues & State Files\"]\n        Q1[job_fetcher_state.json]\n        Q2[state_job_matcher.json]\n        Q3[resume_queue.json]\n        Q4[cover_letter_queue.json]\n        Q5[github_ingestion_state.json]\n    end\n\n    subgraph DB[\"PostgreSQL + pgvector\"]\n        T1[(jobs)]\n        T2[(artifacts)]\n        T3[(generated_artifacts)]\n    end\n\n    subgraph External[\"External Services\"]\n        JAPI[(Job APIs / RSS / Boards)]\n        GH[(GitHub Repos)]\n        LLM[(OpenAI GPT Models)]\n    end\n\n    subgraph Infra[\"Infrastructure\"]\n        DKR[docker-compose Postgres]\n        AWS[AWS scaffolding (future ECS/RDS)]\n    end\n\n    %% UI &lt;-&gt; API (connect to actual nodes, not the subgraph name)\n    A --&gt;|HTTP requests| R1\n    R1 --&gt;|REST/WebSockets| A\n\n    %% API &lt;-&gt; DB\n    R1 --&gt; T1\n    R2 --&gt; T2\n    R3 --&gt; T3\n\n    %% Job ingestion pipeline\n    AF --&gt;|gets jobs| JAPI\n    AF --&gt; Q1\n    Q1 --&gt; AM\n    AM --&gt; Q2\n    AM --&gt;|strong matches| Q3\n    Q3 --&gt; AR\n    AR --&gt; Q4\n    Q4 --&gt; AC\n\n    %% Agents write to DB\n    AF --&gt; T1\n    AM --&gt; T1\n    AR --&gt; T3\n    AC --&gt; T3\n\n    %% GitHub ingestion\n    AG --&gt; GH\n    AG --&gt; Q5\n    Q5 --&gt; R4\n    R4 --&gt; LLM\n\n    %% LLM usage\n    AR --&gt;|prompting| LLM\n    AC --&gt;|prompting| LLM\n    AM --&gt;|LLM skill check| LLM\n\n    %% Infrastructure\n    DKR --&gt; T1\n    DKR --&gt; T2\n    DKR --&gt; T3\n\n    %% Correct dotted-arrow labeling syntax\n    AWS -.-&gt;|future deploy| R1\n    AWS -.-&gt;|future deploy| AF"
  },
  {
    "objectID": "index.html#featured-projects",
    "href": "index.html#featured-projects",
    "title": "Darwhin Gomez",
    "section": "Featured Projects",
    "text": "Featured Projects\nA selection of projects that demonstrate my experience in data engineering, analytics, machine learning, and AI-powered systems.\n\nAgent0 â€“ RAG-Grounded Analytics Agent\nAgent0 is a retrieval-augmented analytics agent that converts natural language questions into safe, explainable SQL insights.\nThe system combines a star-schema analytics warehouse with business-rule and metadata retrieval (RAG) to ensure that generated queries are grounded in approved definitions before execution. Agent0 uses an agentic workflow to decide when to retrieve context, execute SQL, or perform post-processing in Python.\nKey ideas behind Agent0:\n\nSemantic grounding via RAG to reduce SQL hallucinations\n\nClear separation of reasoning, retrieval, and execution\n\nOLAP-first design with a migration path to Postgres or cloud warehouses\n\nGuardrails and auditability beyond prompt-only approaches\n\nTech stack: DuckDB (prototype OLAP) â†’ Postgres/Redshift-style warehouse, FAISS (vector retrieval), LangChain (agent orchestration), Python (analytics + SQL), LLM-agnostic (currently using GPT-4o via OpenAI API).\n\nRepo: https://github.com/DW8888/agent0\n\nArchitecture & design: Agent0 details\n\n\n\n\nAlfred â€“ AI Job Assistant (RAG)\nAlfred originated as both a practical solution to streamline my job search and as a Masterâ€™s capstone project. It is a RAG-powered job application assistant that generates tailored resumes and cover letters based on user profiles and job descriptions.\nKey ideas behind Alfred:\n\nUses vector databases and cosine similarity to retrieve relevant profile and job information\n\nLeverages LLMs to generate customized application materials\n\nBuilt with FastAPI and PostgreSQL/pgvector for efficient retrieval\n\nIncludes a React/Next.js frontend with a FastAPI backend for end-to-end user interaction\n\nTech stack: FastAPI, PostgreSQL/pgvector, Python, Next.js/React (TypeScript), OpenAI-powered RAG workflows.\n\nRepo: https://github.com/DW8888/alfred\nDesign: Alfred flow\n\n\n\n\nFraud Isolation Case Study\nThis project was developed as part of a case study in my data science program, where I built an Isolation Forest model to detect fraudulent financial transactions.\nThe notebook includes data preprocessing, exploratory data analysis, feature engineering, model training, and evaluation â€” demonstrating my ability to apply machine learning techniques to real-world problems.\nTech stack: Python, Jupyter Notebook, scikit-learn, pandas, matplotlib, seaborn.\nNotebook: - Fraud Isolation Case Study\n\n\n\nLinks\n\nAbout: About me\nLinkedIn: https://www.linkedin.com/in/darwhin-gomez\n\nGitHub: https://github.com/dw8888\n\nResume: Resume\nCertifications: Certifications",
    "crumbs": [
      "Home"
    ]
  }
]